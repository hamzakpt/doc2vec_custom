{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evolving vector-space model\n",
    "\n",
    "This lab will be devoted to the use of `doc2vec` model for the needs of information retrieval and text classification.  \n",
    "\n",
    "## 1. Searching in the curious facts database\n",
    "The facts dataset is given [here](https://github.com/hsu-ai-course/hsu.ai/blob/master/code/datasets/nlp/facts.txt), take a look.  We want you to retrieve facts relevant to the query, for example, you type \"good mood\", and get to know that Cherophobia is the fear of fun. For this, the idea is to utilize document vectors. However, instead of forming vectors with tf-idf and reducing dimensions, this time we want to obtain fixed-size vectors for documents using `doc2vec` model.\n",
    "\n",
    "### 1.1 Loading trained `doc2vec` model\n",
    "\n",
    "First, let's load the pre-trained `doc2vec` model from https://github.com/jhlau/doc2vec (Associated Press News DBOW (0.6GB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gensim.models.doc2vec.Doc2Vec'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hmzakpt/.local/lib/python3.6/site-packages/gensim/models/doc2vec.py:566: UserWarning: The parameter `iter` is deprecated, will be removed in 4.0.0, use `epochs` instead.\n",
      "  warnings.warn(\"The parameter `iter` is deprecated, will be removed in 4.0.0, use `epochs` instead.\")\n",
      "/home/hmzakpt/.local/lib/python3.6/site-packages/gensim/models/doc2vec.py:570: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "\n",
    "# unpack a model into 3 files and target the main one\n",
    "# doc2vec.bin  <---------- this\n",
    "# doc2vec.bin.syn0.npy\n",
    "# doc2vec.bin.sin1neg.npy\n",
    "model = Doc2Vec.load('doc2vec.bin', mmap=None)\n",
    "print(type(model))\n",
    "print(type(model.infer_vector([\"to\", \"be\", \"or\", \"not\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Reading data\n",
    "\n",
    "Now, let's read the facts dataset. Download it from the abovementioned url and read to the list of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#TODO read facts into list\n",
    "facts = []\n",
    "with open(\"facts.txt\", \"rb\") as file:\n",
    "    facts= file.read().decode(errors=\"ignore\").split(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. If you somehow found a way to extract all of the gold from the bubbling core of our lovely little planet, you would be able to cover all of the land in a layer of gold up to your knees.\n",
      "2. McDonalds calls frequent buyers of their food heavy users.\n",
      "3. The average person spends 6 months of their lifetime waiting on a red light to turn green.\n",
      "4. The largest recorded snowflake was in Keogh, MT during year 1887, and was 15 inches wide.\n",
      "5. You burn more calories sleeping than you do watching television.\n"
     ]
    }
   ],
   "source": [
    "print(*facts[:5], sep='\\n')\n",
    "\n",
    "assert len(facts) == 159\n",
    "assert ('our lovely little planet') in facts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4  Transforming sentences to vectors\n",
    "\n",
    "Transform the list of facts to numpy array of vectors corresponding to each document (`sent_vecs`), inferring them from the model we just loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO infer vectors\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "sent_vecs =[]\n",
    "pun_trans = str.maketrans('', '', string.punctuation)\n",
    "remove_digits = str.maketrans('', '', string.digits)\n",
    "for para in facts:\n",
    "    para = para.translate(pun_trans) \n",
    "    para = para.translate(remove_digits) \n",
    "    words = nltk.word_tokenize(para)\n",
    "    sent_vecs.append(model.infer_vector(words))\n",
    "sent_vecs = np.array(sent_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159, 300)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_vecs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Tests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159, 300)\n"
     ]
    }
   ],
   "source": [
    "print(sent_vecs.shape)\n",
    "assert sent_vecs.shape == (159, 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Find closest\n",
    "\n",
    "Now, reusing the code from the last lab, find facts which are closest to the query using cosine similarity measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "def find_k_closest(query, dataset, k=5):    \n",
    "    #TODO write here the code that will find 5 closest rows in dataset in terms of cosine similarity\n",
    "    #HINT: as vectors in dataset are already normed, cosine similarity is just dot product.\n",
    "    temp_df = pd.DataFrame(dataset)\n",
    "    temp_df[\"similarity\"] =  cosine_similarity([query], dataset)[0]\n",
    "    matches = []\n",
    "    for top_match in temp_df.sort_values(by=['similarity'], ascending=False).index[:5]:\n",
    "        matches.append([top_match, dataset[top_match, :], temp_df[\"similarity\"][top_match]])\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for query: good mood\n",
      "\t 144. Dolphins sleep with one eye open! sim= 0.6341592\n",
      "\t 68. Cherophobia is the fear of fun. sim= 0.59027797\n",
      "\t 118. An ostrichs eye is bigger than its brain sim= 0.5770811\n",
      "\t 57. Gorillas burp when they are happy sim= 0.5745027\n",
      "\t 110. Cats have 32 muscles in each of their ears. sim= 0.57116723\n"
     ]
    }
   ],
   "source": [
    "#TODO output closest facts to the query\n",
    "query = \"good mood\"\n",
    "# Convert the query words into vector \n",
    "query = query.translate(pun_trans) \n",
    "query = query.translate(remove_digits)\n",
    "query_vector = model.infer_vector(nltk.word_tokenize(query))\n",
    "r = find_k_closest(query_vector, sent_vecs)\n",
    "print(\"Results for query:\", query)\n",
    "for k, v, p in r:\n",
    "    print(\"\\t\", facts[k], \"sim=\", p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training doc2vec model and documents classifier\n",
    "\n",
    "Now we would like you to train doc2vec model yourself based on [this topic-modeling dataset](https://code.google.com/archive/p/topic-modeling-tool/downloads).\n",
    "\n",
    "### 2.1 Read dataset\n",
    "\n",
    "First, read the dataset - it consists of 4 parts, you need to merge them into single list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO read the dataset into list\n",
    "all_data = []\n",
    "def read_txt_dataset(file_name):\n",
    "    with open(file_name, \"rb\") as file:\n",
    "        file_data = file.read().decode(errors=\"ignore\").split(\"\\n\")\n",
    "        if file_data[-1].strip()==\"\":\n",
    "            del file_data[-1]\n",
    "        return file_data\n",
    "all_data += read_txt_dataset(\"testdata_braininjury_10000docs.txt\")\n",
    "all_data += read_txt_dataset(\"testdata_news_economy_2073docs.txt\")\n",
    "all_data += read_txt_dataset(\"testdata_news_fuel_845docs.txt\")\n",
    "all_data += read_txt_dataset(\"testdata_news_music_2084docs.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Tests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15002\n"
     ]
    }
   ],
   "source": [
    "print(len(all_data))\n",
    "assert len(all_data) == 15002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Training `doc2vec` model\n",
    "\n",
    "Train a `doc2vec` model based on the dataset you've loaded. The example of training is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['human', 'interface', 'computer'], ['survey', 'user', 'computer', 'system', 'response', 'time'], ['eps', 'user', 'interface', 'system'], ['system', 'human', 'system', 'eps'], ['user', 'response', 'time'], ['trees'], ['graph', 'trees'], ['graph', 'minors', 'trees'], ['graph', 'minors', 'survey']] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.01329018  0.04400817  0.08257095  0.04851411 -0.07365471]\n"
     ]
    }
   ],
   "source": [
    "#TODO change this according to the task\n",
    "# small set of tokenized sentences\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "# just a test set of tokenized sentences\n",
    "print(common_texts, \"\\n\")\n",
    "corpus = []\n",
    "\n",
    "for doc in all_data:\n",
    "    corpus.append(nltk.word_tokenize(doc))\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(corpus)]\n",
    "print(documents, \"\\n\")\n",
    "# train a model\n",
    "model = Doc2Vec(\n",
    "    documents,     # collection of texts\n",
    "    vector_size=5, # output vector size\n",
    "    window=2,      # maximum distance between the target word and its neighboring word\n",
    "    min_count=1,   # minimal number of \n",
    "    workers=4      # in parallel\n",
    ")\n",
    "\n",
    "# clean training data\n",
    "model.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "\n",
    "# save and load\n",
    "model.save(\"d2v.model\")\n",
    "model = Doc2Vec.load(\"d2v.model\")\n",
    "\n",
    "vec = model.infer_vector([\"system\", \"response\"])\n",
    "print(vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Form train and test datasets\n",
    "\n",
    "Transform documents to vectors and split data to train and test sets. Make sure that the split is stratified as the classes are imbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate labels\n",
    "# braininjury = 0\n",
    "# news_economy = 1\n",
    "# news_fuel = 2\n",
    "# news_music = 3\n",
    "\"\"\"all_data += read_txt_dataset(\"testdata_braininjury_10000docs.txt\")\n",
    "all_data += read_txt_dataset(\"testdata_news_economy_2073docs.txt\")\n",
    "all_data += read_txt_dataset(\"testdata_news_fuel_845docs.txt\")\n",
    "all_data += read_txt_dataset(\"testdata_news_music_2084docs.txt\")\"\"\"\n",
    "y = []\n",
    "for i in range(10000):\n",
    "    y.append(0)\n",
    "for i in range(10000, 10000+2073):\n",
    "    y.append(1)\n",
    "\n",
    "for i in range(10000+2073, 10000+2073+845):\n",
    "    y.append(2)\n",
    "for i in range(10000+2073+845, 10000+2073+845+2084):\n",
    "    y.append(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len :  15002\n"
     ]
    }
   ],
   "source": [
    "#TODO transform and make a train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "corpus_vectors = []\n",
    "for para in all_data:\n",
    "    para = para.translate(pun_trans) \n",
    "    para = para.translate(remove_digits) \n",
    "    words = nltk.word_tokenize(para)\n",
    "    corpus_vectors.append(model.infer_vector(words))\n",
    "print(\"len : \", len(corpus_vectors))\n",
    "X_train, X_test, y_train, y_test = train_test_split(corpus_vectors, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Train topics classifier\n",
    "\n",
    "Train a classifier that would classify any document to one of four categories: fuel, brain injury, music, and economy.\n",
    "Print a classification report for test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score :  0.8770409863378874\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      2020\n",
      "           1       0.59      0.73      0.65       399\n",
      "           2       0.37      0.19      0.25       197\n",
      "           3       0.74      0.75      0.74       385\n",
      "\n",
      "   micro avg       0.88      0.88      0.88      3001\n",
      "   macro avg       0.67      0.67      0.66      3001\n",
      "weighted avg       0.87      0.88      0.87      3001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#TODO train a classifier and measure its performance\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "neigh.fit(X_train, y_train)\n",
    "y_pred = neigh.predict(X_test)\n",
    "print(\"Accuracy score : \", neigh.score(X_test, y_test))\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which class is the hardest one to recognize?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Bonus task\n",
    "\n",
    "What if we trained our `doc2vec` model using window size = 5 or 10? Would it improve the classification acccuracy? What about vector dimensionality? Does it mean that increasing it we will achieve better performance in terms of classification?\n",
    "\n",
    "Explore the influence of these parameters on classification performance, visualizing it as a graph (e.g. window size vs f1-score, vector dim vs f1-score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_doc2vec(documents, vector_size, window, model_name=\"d2v.model\"):\n",
    "    model = Doc2Vec(\n",
    "        documents,     # collection of texts\n",
    "        vector_size=vector_size, # output vector size\n",
    "        window=window,      # maximum distance between the target word and its neighboring word\n",
    "        min_count=1,   # minimal number of \n",
    "        workers=4      # in parallel\n",
    "    )\n",
    "\n",
    "    # clean training data\n",
    "    model.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "\n",
    "    # save and load\n",
    "    model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vector size =5, window size 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"d2v_window5.model\"\n",
    "train_doc2vec(documents, vector_size=5, window=5, model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec.load(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len :  15002\n",
      "Accuracy score :  0.8830389870043319\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      2014\n",
      "           1       0.63      0.69      0.66       409\n",
      "           2       0.37      0.24      0.29       174\n",
      "           3       0.74      0.77      0.76       404\n",
      "\n",
      "   micro avg       0.88      0.88      0.88      3001\n",
      "   macro avg       0.68      0.68      0.68      3001\n",
      "weighted avg       0.88      0.88      0.88      3001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus_vectors = []\n",
    "for para in all_data:\n",
    "    para = para.translate(pun_trans) \n",
    "    para = para.translate(remove_digits) \n",
    "    words = nltk.word_tokenize(para)\n",
    "    corpus_vectors.append(model.infer_vector(words))\n",
    "print(\"len : \", len(corpus_vectors))\n",
    "X_train, X_test, y_train, y_test = train_test_split(corpus_vectors, y, test_size=0.2)\n",
    "\n",
    "\n",
    "\n",
    "neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "neigh.fit(X_train, y_train)\n",
    "y_pred = neigh.predict(X_test)\n",
    "print(\"Accuracy score : \", neigh.score(X_test, y_test))\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vector size =5, window size 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"d2v_window10.model\"\n",
    "train_doc2vec(documents, vector_size=5, window=10, model_name=model_name)\n",
    "model = Doc2Vec.load(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len :  15002\n",
      "Accuracy score :  0.8810396534488504\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      2018\n",
      "           1       0.59      0.70      0.64       398\n",
      "           2       0.38      0.23      0.29       163\n",
      "           3       0.76      0.74      0.75       422\n",
      "\n",
      "   micro avg       0.88      0.88      0.88      3001\n",
      "   macro avg       0.68      0.67      0.67      3001\n",
      "weighted avg       0.88      0.88      0.88      3001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus_vectors = []\n",
    "for para in all_data:\n",
    "    para = para.translate(pun_trans) \n",
    "    para = para.translate(remove_digits) \n",
    "    words = nltk.word_tokenize(para)\n",
    "    corpus_vectors.append(model.infer_vector(words))\n",
    "print(\"len : \", len(corpus_vectors))\n",
    "X_train, X_test, y_train, y_test = train_test_split(corpus_vectors, y, test_size=0.2)\n",
    "\n",
    "\n",
    "\n",
    "neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "neigh.fit(X_train, y_train)\n",
    "y_pred = neigh.predict(X_test)\n",
    "print(\"Accuracy score : \", neigh.score(X_test, y_test))\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vector size =50, window size 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"d2v_vector50.model\"\n",
    "train_doc2vec(documents, vector_size=50, window=2, model_name=model_name)\n",
    "model = Doc2Vec.load(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len :  15002\n",
      "Accuracy score :  0.895034988337221\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      1970\n",
      "           1       0.68      0.74      0.71       438\n",
      "           2       0.47      0.31      0.38       153\n",
      "           3       0.79      0.78      0.78       440\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      3001\n",
      "   macro avg       0.73      0.71      0.72      3001\n",
      "weighted avg       0.89      0.90      0.89      3001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus_vectors = []\n",
    "for para in all_data:\n",
    "    para = para.translate(pun_trans) \n",
    "    para = para.translate(remove_digits) \n",
    "    words = nltk.word_tokenize(para)\n",
    "    corpus_vectors.append(model.infer_vector(words))\n",
    "print(\"len : \", len(corpus_vectors))\n",
    "X_train, X_test, y_train, y_test = train_test_split(corpus_vectors, y, test_size=0.2)\n",
    "\n",
    "\n",
    "\n",
    "neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "neigh.fit(X_train, y_train)\n",
    "y_pred = neigh.predict(X_test)\n",
    "print(\"Accuracy score : \", neigh.score(X_test, y_test))\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vector size =100, window size 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"d2v_vector100.model\"\n",
    "train_doc2vec(documents, vector_size=100, window=2, model_name=model_name)\n",
    "model = Doc2Vec.load(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len :  15002\n",
      "Accuracy score :  0.9046984338553815\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      2014\n",
      "           1       0.69      0.77      0.73       398\n",
      "           2       0.57      0.37      0.45       171\n",
      "           3       0.80      0.79      0.80       418\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      3001\n",
      "   macro avg       0.76      0.73      0.74      3001\n",
      "weighted avg       0.90      0.90      0.90      3001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus_vectors = []\n",
    "for para in all_data:\n",
    "    para = para.translate(pun_trans) \n",
    "    para = para.translate(remove_digits) \n",
    "    words = nltk.word_tokenize(para)\n",
    "    corpus_vectors.append(model.infer_vector(words))\n",
    "print(\"len : \", len(corpus_vectors))\n",
    "X_train, X_test, y_train, y_test = train_test_split(corpus_vectors, y, test_size=0.2)\n",
    "\n",
    "\n",
    "\n",
    "neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "neigh.fit(X_train, y_train)\n",
    "y_pred = neigh.predict(X_test)\n",
    "print(\"Accuracy score : \", neigh.score(X_test, y_test))\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vector size =100, window size 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"d2v_vector100_win5.model\"\n",
    "train_doc2vec(documents, vector_size=100, window=5, model_name=model_name)\n",
    "model = Doc2Vec.load(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len :  15002\n",
      "Accuracy score :  0.9003665444851716\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      1999\n",
      "           1       0.66      0.76      0.71       406\n",
      "           2       0.55      0.43      0.48       160\n",
      "           3       0.83      0.75      0.79       436\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      3001\n",
      "   macro avg       0.76      0.73      0.74      3001\n",
      "weighted avg       0.90      0.90      0.90      3001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus_vectors = []\n",
    "for para in all_data:\n",
    "    para = para.translate(pun_trans) \n",
    "    para = para.translate(remove_digits) \n",
    "    words = nltk.word_tokenize(para)\n",
    "    corpus_vectors.append(model.infer_vector(words))\n",
    "print(\"len : \", len(corpus_vectors))\n",
    "X_train, X_test, y_train, y_test = train_test_split(corpus_vectors, y, test_size=0.2)\n",
    "\n",
    "\n",
    "\n",
    "neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "neigh.fit(X_train, y_train)\n",
    "y_pred = neigh.predict(X_test)\n",
    "print(\"Accuracy score : \", neigh.score(X_test, y_test))\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
